---
title: "STAT 5310 : Project"
subtitle: "A Study on Female Participation in Workforce in World"
output: html_notebook
font: 10pt
---

# Introduction

Half of the world's population roughly comprises of women but when compared to a country’s total workforce the male and female workers percentage is rarely similar. If you look at the developing and underdeveloped countries, it’s even more prominent. Insufficient access to education, religious superstitions, lack of adequate infrastructures are some of the reasons responsible for this discrepancy, also it goes way beyond these.  The total labor force has been considered to show the effects of multiple socioeconomic factors on the women participation in the total workforce ad percentage of female employment. The relationship between these factors can be analyzed using multiple linear regression model.

# Problem 
Our original data comes from World Bank database where they have a collection of development indicators to estimate various socio-economic factors for all nations in the world. The data was collected using the Data Bank online resource which allows users to form custom time-series data sets based on chosen filters like countries, years and development indicators. We gathered data for each of the 11 development indicators (including the employed women percentage and related predictor variables) across 217 countries for the most recent year, 2019. After performing data preprocessing like removing missing values and labeling our indicators to more simple variable names .etc , we get our final data having 187 data points (countries). There is one response variable which is the percentage of the employed women explanatory variables of predictors. Brief descriptions of these variables are given below.

**1. PerFemEmploy (Employment to population ratio (%) of women who are of age 15 or older.)** Employment to population ratio is the proportion of a country's population that is employed. Employment is defined as persons of working age who, during a short reference period, were engaged in any activity to produce goods or provide services for pay or profit, whether at work during the reference period (i.e. who worked in a job for at least one hour) or not at work due to temporary absence from a job, or to working-time arrangements. Ages 15 and older are generally considered the working-age population.

**2. FertilityRate (Fertility rate (birth per women).)** Total fertility rate represents the number of children that would be born to a woman if she were to live to the end of her childbearing years and bear children in accordance with age-specific fertility rates of the specified year.

**3. RatioMaletoFemale Ratio of female to male labor force participation rate.** Labor force participation rate is the proportion of the population ages 15 and older that is economically active: all people who supply labor for the production of goods and services during a specified period. Ratio of female to male labor force participation rate is calculated by dividing female labor force participation rate by male labor force participation rate and multiplying by 100.

**4. PerFemEmployers Employers, female (% of female employment).** Employers are those workers who, working on their own account or with one or a few partners, hold the type of jobs defined as a "self-employment jobs" i.e. jobs where the remuneration is directly dependent upon the profits derived from the goods and services produced), and, in this capacity, have engaged, on a continuous basis, one or more persons to work for them as employee(s).

**Agriculture Employment in agriculture, female (% of female employment).** Employment is defined as persons of working age who were engaged in any activity to produce goods or provide services for pay or profit, whether at work during the reference period or not at work due to temporary absence from a job, or to working-time arrangement. The agriculture sector consists of activities in agriculture, hunting, forestry and fishing, in accordance with division 1 (ISIC 2) or categories A-B (ISIC 3) or category A (ISIC 4).

**5. Industry Employment in industry, female (% of female employment).** The industry sector consists of mining and quarrying, manufacturing, construction, and public utilities (electricity, gas, and water), in accordance with divisions 2-5 (ISIC 2) or categories C-F (ISIC 3) or categories B-F (ISIC 4).

**6. Services Employment in services, female (% of female employment).** The services sector consists of wholesale and retail trade and restaurants and hotels; transport, storage, and communications; financing, insurance, real estate, and business services; and community, social, and personal services, in accordance with divisions 6-9 (ISIC 2) or categories G-Q (ISIC 3) or categories G-U (ISIC 4).

**7. Wage.Salaried Wage and salaried workers, female (% of female employment).** Wage and salaried workers (employees) are those workers who hold the type of jobs defined as "paid employment jobs," where the incumbents hold explicit (written or oral) or implicit employment contracts that give them a basic remuneration that is not directly dependent upon the revenue of the unit for which they work.

**8. ContrFamWorkers Contributing family workers, female (% of female employment).** Contributing family workers are those workers who hold "self-employment jobs" as own-account workers in a market-oriented establishment operated by a related person living in the same household.

**9. OwnAccount Own-account female workers (% of employment).** Own-account workers are workers who, working on their own account or with one or more partners, hold the types of jobs defined as "self-employment jobs" and have not engaged on a continuous basis any employees to work for them. Own account workers are a subcategory of "self-employed".

**10. Vulnerable Vulnerable employment, female (% of female employment).** Vulnerable employment is contributing family workers and own-account workers as a percentage of total employment.

# Purpose

We can apply Linear Regression Model and other statistical methods on this dataset to analyze if there are any viable relationship between the response of the variables and the predictor.

# Methodology

## A. Data Preprocessing
```{r,include=FALSE}
##Import libraries here
library(tidyverse)
library(car)
library(corrplot)
library(grid)
library(gridExtra)
library(repr)
library(olsrr)
library(broom)
library(SciViews)
library(MASS)
library(MPV)
library(cvTools)
library("reshape2") 
library(maps) 
```


```{r,include=FALSE}
##Import data
df<- read_csv("data/93a9675f-63e9-4a5c-84ae-389e7022a50d_Data.csv",show_col_types = FALSE)
attach(df)
```
```{r,include=FALSE}
##Untidy Data
head(df)
```

```{r,include=FALSE}
## Adding ids as primary key
df <- df %>% mutate(id=row_number()) %>% dplyr::select(id,everything())
```






```{r,include=FALSE}
## Store Indicators. Drop from df.
IndicatorNames = data.frame('Indicator_Name' =unique(`Series Name`),'Indicator_Var'=unique(`Series Code`) ) 
```

### A.1. Labelling Variable Names

We converted the indicator names to more simple and appropriate variable names. 

```{r,include=FALSE}
##Simplifying the indicator names here
old_val= unique(`Series Code`)
new_val= c('Wage.Salaried', 'Vulnerable', 'OwnAccount',  'RatioMaletoFemale',  'FertilityRate','PerFemEmploy','Services', 'Industry','Agriculture','PerFemEmployers', 'ContrFamWorkers') 

data.frame("Indicator Names" = unique(`Series Name`),"Variable Names"=new_val)

df <- df %>% 
  mutate(Indicators=plyr::mapvalues(`Series Code`, from = old_val, to = new_val)) %>%
  dplyr::select(c(-`Series Name`,-`Series Code`))


```


```{r,include=FALSE}
## Unique columns for Indicators for each Country
coalesce_by_column <- function(df) {
  return(dplyr::coalesce(!!! as.list(df)))
}

df<- df  %>%  
    spread(key=`Indicators`, value=`2019 [YR2019]`) %>% 
    group_by(`Country Code`) %>%
    summarise_all(coalesce_by_column) %>% 
    dplyr::select(id,`Country Code`,`Country Name`,'PerFemEmploy',everything())


```
```{r,include=FALSE}
df[4:14] <- lapply(df[4:14], FUN = function(y){as.numeric(y)})   ##Convert datatype to numeric
```

### A.2. Missing Values
```{r,echo=FALSE}
## Check for nas
colSums(is.na(df))
```

```{r,include=FALSE}
df<- na.omit(df)  #Removing NA values
```


```{r,include=FALSE}
## Store Country Names. Drop from df.
CountryNames = unique(df['Country Name'])$'Country Name'
```

```{r,include=FALSE}
## Final Dataset
head(df)                        ## 187 rows x 13 columns
```

## A.3. Outlier Analysis
```{r,echo=FALSE}
suppressMessages(df_main <- melt(df[4:14]))
levels(df_main$variable) <- names(df[4:14])
ggplot(df_main, aes(variable,value)) + geom_boxplot() + coord_flip()
```


## B. Exploratory Data Analysis

Let's see if the data meets the first assumption for regression analysis i.e. linear relationship of response with at least one of the regressors. The exploratory analysis will help to reveal relationship between the response and the regressor variables. The obtained results can help us narrow down our search for potential predictors that have significant effect on determining the percentage of employed women for a nation.

### B.1. Correlation Visualization
We can see correlations between the variables by visualizing though correlation plot.

```{r,echo=FALSE}
##Visualize Correlations 
corrplot(cor(df[, 4:14]), type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

Our response *PerFemEmploy* is highly positively correlated with independent variable *RatioMaletoFemale*. Some independent variables seem to be highly correlated (for eg. *OwnAccount* is highly correlated with *Vulnerable* (+),*Agriculture* (+), *Services* (-) and *Wage.Salaried* (-) .etc). We need to have a closer look at these variables to determine if multicollinearity is present in data models.

```{r,echo=FALSE}
df_c=df
colnames(df_c)[3] <- "region"



world <- map_data("world") %>% 
  filter(! long > 180)

world["region"][world["region"] == "USA"] <- "United States"
world["region"][world["region"] == "Russia"] <- "Russian Federation"
world["region"][world["region"] == "Brunei"] <- "Brunei Darussalam"
world["region"][world["region"] == "Egypt"] <- "Egypt, Arab Rep."
world["region"][world["region"] == "UK"] <- "United Kingdom"
world["region"][world["region"] == "South Korea"] <- "Korea, Rep."
world["region"][world["region"] == "Laos"]<- "Lao PDR"
world["region"][world["region"] == "North Korea"] <- "Korea, Dem. People's Rep."
world["region"][world["region"] == "Slovakia"] <- "Slovak Republic"
world["region"][world["region"] == "Syria"] <- "Syrian Arab Republic"
world["region"][world["region"] == "Saint Vincent"] <- "St. Vincent and the Grenadines"
world["region"][world["region"] == "Grenadines"] <- "St. Vincent and the Grenadines"
world["region"][world["region"] == "Venezuela"] <- "Venezuela, RB"
world['region'][which(world$region %like% "Virgin Islands"  & world$subregion %like% "US"),] <- "Virgin Islands (U.S.)"
world["region"][world["region"] == "Yemen"] <- "Yemen, Rep."
world["region"][world["region"] == "Bahamas"] <- "Bahamas, The"
world["region"][world["region"] == "Democratic Republic of the Congo"] <- "Congo, Dem. Rep."
world["region"][world["region"] == "Republic of Congo"] <- "Congo, Rep."
world["region"][world["region"] == "Cape Verde"] <- "Cabo Verde"
world["region"][world["region"] == "Gambia"] <- "Gambia, The"
world["region"][world["region"] == "Kyrgyzstan"] <- "Kyrgyz Republic"

merged_df <- world[world$region %in% df_c$region, ]
merged_df$PerFemEmploy <- df_c$PerFemEmploy[match(merged_df$region, df_c$region)]


suppressMessages(
ggplot() +
    geom_map(data = world, map = world,
                  aes(x = long, y = lat, group = group, map_id=region)
                   , size=0.01) + 
    geom_map(data =  merged_df, map=world,
                  aes(fill=PerFemEmploy, map_id=region),
                   size=0.01) +
    coord_map("moll",) +
    scale_fill_continuous(low="mistyrose", high="deeppink", guide="colorbar") +
    scale_y_continuous(breaks=c()) +
    scale_x_continuous(breaks=c()) +
    labs(fill="legend", title="Percentage of Employed Women", x="", y="") +
    theme(text = element_text( color = "#FFFFFF")
        ,panel.background = element_rect(fill = "azure4")
        ,plot.background = element_rect(fill = "azure4")
        ,panel.grid = element_blank()
        ,plot.title = element_text(size = 30)
        ,plot.subtitle = element_text(size = 10)
        ,axis.text = element_blank()
        ,axis.title = element_blank()
        ,axis.ticks = element_blank()
        ,legend.position = "none"
        ))
```


```{r,include=FALSE}
par(mfrow = c(2, 2))
#Histograms
plot_list<- list()
for(col in names(df[4:14])){
  hist(df[[col]],main=paste("Histogram for",col),xlab=col)
  #print(g)
}


```


```{r,include=FALSE}
par(mfrow = c(2,3))
# scatterplot using df
s1 <- ggplot(data = df, aes(x = Agriculture, y = PerFemEmploy)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(y= " PerFemEmploy", 
       x="Agriculture") +
  theme(plot.title = element_text(hjust = 0.5))

s2 <- ggplot(data = df, aes(x = ContrFamWorkers, y = PerFemEmploy)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(y= " PerFemEmploy", 
       x="ContrFamWorkers") +
  theme(plot.title = element_text(hjust = 0.5))

s3 <- ggplot(data = df, aes(x =  FertilityRate, y = PerFemEmploy)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(y= " PerFemEmploy", 
       x=" FertilityRate") +
  theme(plot.title = element_text(hjust = 0.5))

s4 <- ggplot(data = df, aes(x = Industry, y = PerFemEmploy)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(y= " PerFemEmploy", 
       x="Industry") +
  theme(plot.title = element_text(hjust = 0.5))

s5 <- ggplot(data = df, aes(x = OwnAccount, y = OwnAccount)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(y= " PerFemEmploy", 
       x="OwnAccount") +
  theme(plot.title = element_text(hjust = 0.5))

s6 <- ggplot(data = df, aes(x = PerFemEmployers, y = PerFemEmploy)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(y= " PerFemEmploy", 
       x="PerFemEmployers") +
  theme(plot.title = element_text(hjust = 0.5))

s7 <- ggplot(data = df, aes(x = RatioMaletoFemale, y = PerFemEmploy)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(y= " PerFemEmploy", 
       x="RatioMaletoFemale") +
  theme(plot.title = element_text(hjust = 0.5))

s8 <- ggplot(data = df, aes(x = Services, y = PerFemEmploy)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(y= " PerFemEmploy", 
       x="Services") +
  theme(plot.title = element_text(hjust = 0.5))

s9 <- ggplot(data = df, aes(x = Vulnerable, y = PerFemEmploy)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(y= " PerFemEmploy", 
       x="Vulnerable") +
  theme(plot.title = element_text(hjust = 0.5))

s10 <- ggplot(data = df, aes(x = Agriculture, y = Wage.Salaried)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(y= " PerFemEmploy", 
       x="Wage.Salaried") +
  theme(plot.title = element_text(hjust = 0.5))

##Arrange the Plots
grid.arrange(s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,nrow=2)
```



## Modelling
```{r}
## Initial Full Model with all 10 predictor variables
full.fit<- lm(PerFemEmploy~.,df[4:14])
summary.aov(full.fit)
```
The predictors *Agriculture* , *FertilityRate* , *Industry*, *OwnAccount*, *RatioMaletoFemale* are significant for this model (p-values less than 0.05).

```{r}
summary.lm(full.fit)
```
- For our full model, we get p-value of 2.2e-16, which is very less than $\alpha$. We reject the null hypothesis and say this regression model is significant to be considered. 
- The $R^2$ (0.80) and $R^2_{Adj}$ (0.79) values are good enough. But these high values may also be due to more variables used in the model.  
- The coefficient estimates are high and we might need to perform centering of the regressors.

```{r}
df_new<- df[4:14]
bc <- boxcox(lm(PerFemEmploy~.,df_new))
lambda <- bc$x[which.max(bc$y)]
lambda                                          ## Try sqrt(y+c) transformation

bc$x[bc$y > max(bc$y) - 1/2 * qchisq(.95,1)]    ## Does not contain 1
```
The Box-Cox transformation suggests that we transform our response before commencing. But let's check our model assumptions before so we can see if it meets constant variance assumption.

### Diagnostics Residual Analysis

```{r}
##LINEARITY
## Residuals Vs Fit Plot
plot(full.fit,1)
## Interpretation: Red line close to Horizontal. Linearity is present.
```
```{r}
#INDEPENDENCE
lmtest::dwtest(full.fit) 
##Interpretation : Error terms are independent.
```

```{r}
##NORMALITY
## Normality plot
plot(full.fit,2)
shapiro.test(full.fit$residuals)  
##Interpretation: The data is normal.
##
```

```{r}
##EQUAL VARIANCE
##Scale-Location Plot
plot(full.fit, 3)
lmtest::bptest(full.fit)   ##Retain null. Constant Variance
##Points Equally Spread Out. But red fitted line is not horizontal (seems to increase). 
##No need to transform y.
```


```{r}
## Outliers and Leverages
plot(full.fit, 5)

## Interpretation: Points #15,#73,#89 are extreme values with standardized residuals beyond absolute value of 2. No points are exceed 3 standard deviations, which is good. There are leverage points i.e. some points have leverage statistic beyond 2(p + 1)/n = 22/187 = 0.117. 
```
```{r}
#influence.measures(model)  ##Possible Influential Points 1,9,13,65,73,89,104,126,129,133,141,152,154,160,168,183
ols_plot_resid_lev(full.fit)
plot(full.fit,4)
## 89,73,154 are leverage points
## No influential points as per Cook's distance
```
```{r}
#plot(lm(PerFemEmploy~.,df_c),5)
#plot(lm(PerFemEmploy~.,df_c[-89,-73,-154]),5)
##Not very significant impact after removing leverage points.
##Decision: Do not discard points from regression analysis.
```

```{r}
##Variance Inflation Factors
barplot(vif(full.fit), main = "VIF Values", horiz = TRUE, col = "steelblue",las=1) #Very high multicollinearity
```

## Finding Best Model

#### Using Best Subsets Regression Method
```{r}
full.fit<- lm(PerFemEmploy~.,df[4:14])
best.mods<-ols_step_best_subset(full.fit)
best.mods

plot(best.mods)
```


#### Using Forward Regression Method
```{r}
fwd.fit.p<- ols_step_forward_p(full.fit,0.05)
fwd.fit.p
```
```{r}
par(mfrow=c(2,2))
## Obtained from Fwd Regression using p-value
m0<- lm(PerFemEmploy~RatioMaletoFemale+ContrFamWorkers,df[4:14])
## Present in our Best Subsets Models Table

plot(m0,2)            #Normality
ols_test_normality(m0) #Reject H0 at 5% significance level for 3 tests.

plot(m0,1)      ##Residuals Vs Fitted Plot
lmtest::bptest(m0) ## Equal Variances

vif(m0)     #No multicollinearity

PRESS(m0)  ##PRESS statistic

sst<- sum(anova(m0)$'Sum Sq')
(pred.r.squared <- 1-PRESS(m0)/(sst))    ##Predictive R-squared = 78.7%

car::avPlots(m0)      ##Check if model can be linearized by adding non-linear terms
```


```{r}
##Forward Regression Method using AIC
fwd.fit.aic <- ols_step_forward_aic(full.fit)
fwd.fit.aic 

plot(fwd.fit.aic )
```

```{r}
par(mfrow=c(2,2))
## Obtained from Fwd Regression using AIC
m1<- lm(PerFemEmploy~RatioMaletoFemale+FertilityRate+ContrFamWorkers+PerFemEmployers+Industry,df[4:14])
  
plot(m1,2)            #Normality
ols_test_normality(m1) #Reject H0 at 5% significance level for 3 tests.

plot(m1,c(1))      ##Residuals Vs Fitted Plot
lmtest::bptest(m1) ## Equal Variances

vif(m1)     #No multicollinearity

PRESS(m1)  ##PRESS statistic

sst<- sum(anova(m1)$'Sum Sq')
(pred.r.squared <- 1-PRESS(m1)/(sst))    ##Predictive R-squared = 78.91%

car::avPlots(m1)      ##Check if model can be linearized by adding non-linear terms
```

#### Using Backwards Regression Method
```{r}
##Using pvalue 
bwd.fit.p <- ols_step_backward_p(full.fit,0.05)
bwd.fit.p  #Same as m0
```
```{r}
## Using AIC
bwd.fit.aic<- ols_step_backward_aic(full.fit) 
bwd.fit.aic

plot(bwd.fit.aic)      
```
```{r}
par(mfrow=c(2,2))
## Present in our Best Subsets Models Table
m2<- lm(PerFemEmploy~RatioMaletoFemale+FertilityRate+ContrFamWorkers+PerFemEmployers+Industry+Agriculture+Services,df[4:14])
  
plot(m2,2)            #Normality
ols_test_normality(m2) #Reject H0 at 5% significance level for 3 tests.

plot(m2,c(1))      ##Residuals Vs Fitted Plot
lmtest::bptest(m2) ## Unequal Variances

vif(m2)     #Very High multicollinearity

PRESS(m2)  ##PRESS statistic

sst<- sum(anova(m2)$'Sum Sq')
(pred.r.squared <- 1-PRESS(m2)/(sst))    ##Predictive R-squared = 78.6%
```

```{r}
par(mfrow=c(2,2))
##Boxcox transformation
df_new<- df[4:14]
bc <- boxcox(lm(PerFemEmploy~RatioMaletoFemale+FertilityRate+ContrFamWorkers+PerFemEmployers+Industry+Agriculture+Services,df_new))
lambda <- bc$x[which.max(bc$y)]
lambda                                          

bc$x[bc$y > max(bc$y) - 1/2 * qchisq(.95,1)]    

## Modified m2
m2<- lm(((PerFemEmploy^lambda-1)/lambda)~RatioMaletoFemale+FertilityRate+ContrFamWorkers+PerFemEmployers+Industry+Agriculture+Services,df[4:14])

#Normality 
plot(m2,2)            
ols_test_normality(m2) #Reject H0 at 5% significance level for 3 tests.

##Constant Variance Assumption
plot(m2,c(1))      ##Residuals Vs Fitted Plot
lmtest::bptest(m2) ## Equal Variances achieved

vif(m2)     #Very High multicollinearity

PRESS(m2)  ##PRESS statistic

sst<- sum(anova(m2)$'Sum Sq')
(pred.r.squared <- 1-PRESS(m2)/(sst))    ##Predictive R-squared = 80.15%

car::avPlots(m2)      ##Check if model can be linearized by adding non-linear terms
```

#### Using StepWise Regression Method 
```{r}
## p-value approach
step.fit.p<- ols_step_both_p(full.fit,0.05)
step.fit.p ##Same as m0
```
```{r}
step.fit.aic<- ols_step_both_p(full.fit)
step.fit.aic      ##Same as m0
```
```{r}
par(mfrow=c(2,2))

## Other models Present in our Best Subsets Models Table
m3<- lm(PerFemEmploy~RatioMaletoFemale, df[4:14])
  
plot(m3,2)            #Normality
ols_test_normality(m3) #Reject H0 at 5% significance level for 3 tests.

plot(m3,c(1))      ##Residuals Vs Fitted Plot
lmtest::bptest(m3) ## Unequal Variances

#vif(m2)     #No multicollinearity

PRESS(m3)  ##PRESS statistic

sst<- sum(anova(m3)$'Sum Sq')
(pred.r.squared <- 1-PRESS(m3)/(sst))    ##Predictive R-squared = 75.29%
```

```{r}
par(mfrow=c(2,2))
##Boxcox transformation
df_new<- df[4:14]
bc <- boxcox(lm((PerFemEmploy)~RatioMaletoFemale,df_new))
lambda <- bc$x[which.max(bc$y)]
lambda

## Modified m3
m3<- lm(((PerFemEmploy^lambda-1)/lambda) ~ RatioMaletoFemale,df[4:14])

##Normality
plot(m3,2)
ols_test_normality(m3)     #Reject H0 at 5% alpha level according to 3 tests

## Constant Variance
plot(m3,1)
lmtest::bptest(m3)        #Equal Variance achieved

sst<- sum(anova(m3)$'Sum Sq')
(pred.r.squared <- 1-PRESS(m3)/(sst))    ##Predictive R-squared = 78.81%

avPlots(m3)                    #Check if model can be linearized by adding non linear terms
```

```{r}
par(mfrow=c(2,2))
## Other models Present in our Best Subsets Models Table
m4<- lm(PerFemEmploy~ ContrFamWorkers+PerFemEmployers+RatioMaletoFemale, df[4:14])
  
plot(m4,2)            #Normality
ols_test_normality(m4) #Reject H0 at 5% significance level for 2 tests.

plot(m4,c(1))      ##Residuals Vs Fitted Plot
lmtest::bptest(m4) ## Equal Variance

vif(m4)     #No multicollinearity 
PRESS(m4)  ##PRESS statistic

sst<- sum(anova(m4)$'Sum Sq')
(pred.r.squared <- 1-PRESS(m4)/(sst))    ##Predictive R-squared = 78.78%

avPlots(m4)                    #Check if model can be linearized by adding non linear terms
```
```{r}
par(mfrow=c(2,2))
## Other models Present in our Best Subsets Models Table
m5<- lm(PerFemEmploy~ Agriculture+ContrFamWorkers+Industry+PerFemEmployers+RatioMaletoFemale+Services, df[4:14])
plot(m5,2)            #Normality
ols_test_normality(m5) #Reject H0 at 5% significance level for 3 tests.

plot(m5,c(1))      ##Residuals Vs Fitted Plot
lmtest::bptest(m5) ## Unequal Variances
vif(m5)     #High multicollinearity

PRESS(m5)  ##Low PRESS statistic

sst<- sum(anova(m5)$'Sum Sq')
(pred.r.squared <- 1-PRESS(m5)/(sst))    ##Predictive R-squared = 78.54%

```
```{r}
par(mfrow=c(2,2))
##Boxcox transformation
df_new<- df[4:14]
bc <- boxcox(lm((PerFemEmploy)~Agriculture+ContrFamWorkers+Industry+PerFemEmployers+RatioMaletoFemale+Services,df_new))
lambda <- bc$x[which.max(bc$y)]
lambda

##Modified m5
m5<- lm(((PerFemEmploy^lambda-1)/lambda) ~Agriculture+ContrFamWorkers+Industry+PerFemEmployers+RatioMaletoFemale+Services,df[4:14])

##Normality
plot(m5,2)
ols_test_normality(m5)     #Reject H0 at 5% alpha level according to 3 tests

## Constant Variance
plot(m5,1)
lmtest::bptest(m5)        #Equal Variance achieved

vif(m5)                   #High multicollinearity

sst<- sum(anova(m5)$'Sum Sq')
(pred.r.squared <- 1-PRESS(m5)/(sst))    ##Predictive R-squared = 79.8%

avPlots(m5)                    #Check if model can be linearized by adding non linear terms
```


-- To be continued

## Results
Though m2 (with model equation ) has maximum predictive $R^2$ value () and lowest $C_p$ statistic (), it has very high multicollinearity.
- Same is the case for m5 . 
- Our next option is m3 () but might be considered too simple.
- 







**Regression Equation for Estimated Model**
$$y = \beta_0 + \beta_1 x_1 + \beta_0 x_2 + \beta_0 x_3 +$$

